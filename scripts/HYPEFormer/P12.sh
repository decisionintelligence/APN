#!/bin/bash

# =========================================================================
#             HYPE-Former åœ¨ P12 æ•°æ®é›†ä¸Šçš„è¶…å‚æ•°æœç´¢è„šæœ¬
# =========================================================================

# --- 1. GPU é…ç½® ---
GPU_IDS="0,1,2,3,4,5,6,7"
IFS=',' read -r -a GPUS <<< "$GPU_IDS"
NUM_GPUS=${#GPUS[@]}
job_count=0

echo "ğŸš€ å°†åœ¨ ${NUM_GPUS} ä¸ªGPU (${GPUS[*]}) ä¸Šå¹¶è¡Œæ‰§è¡Œ HYPE-Former çš„è¶…å‚æ•°æœç´¢ä»»åŠ¡ã€‚"

# --- 2. å›ºå®šå‚æ•° ---
model_name="HYPEFormer"
dataset_root_path="storage/datasets/P12"
dataset_name="P12"
seq_len=36
pred_len=12
enc_in=36
dec_in=36
c_out=36

# --- 3. HYPE-Former è¶…å‚æ•°æœç´¢ç©ºé—´ ---
# æ¨¡å‹ç»“æ„ç›¸å…³
d_models=(64 128)
n_heads_options=(4 8)
n_layers=(1 2 3)
d_ff_multipliers=(2 4)
dropouts=(0.1 0.2 0.3)

# HYPE-Former æ ¸å¿ƒå‚æ•°
patch_lens=(8 12 16)
strides=(8 12 16) #

# è®­ç»ƒç›¸å…³
batch_sizes=(16 32)
lrs=(0.001 0.0005)

# --- 4. éšæœºæœç´¢è®¾ç½® ---
TOTAL_RUNS=1296
echo "ğŸš€ å¼€å§‹ HYPE-Former åœ¨ P12 ä¸Šçš„éšæœºæœç´¢ï¼Œæ€»å…±å°†è¿è¡Œ ${TOTAL_RUNS} ç»„å®éªŒ..."

# --- 5. éšæœºæœç´¢ä¸»å¾ªç¯ ---
for (( i=1; i<=${TOTAL_RUNS}; i++ )); do

  # --- éšæœºé‡‡æ ·è¶…å‚æ•° ---
  dm=${d_models[$((RANDOM % ${#d_models[@]}))]}
  nl=${n_layers[$((RANDOM % ${#n_layers[@]}))]}
  dff_m=${d_ff_multipliers[$((RANDOM % ${#d_ff_multipliers[@]}))]}
  dff=$((dm * dff_m))
  dp=${dropouts[$((RANDOM % ${#dropouts[@]}))]}
  bs=${batch_sizes[$((RANDOM % ${#batch_sizes[@]}))]}
  lr=${lrs[$((RANDOM % ${#lrs[@]}))]}

  # é‡‡æ · patch_len å’Œ strideï¼Œå¹¶ç¡®ä¿ stride <= patch_len
  pl=${patch_lens[$((RANDOM % ${#patch_lens[@]}))]}
  st=$pl
  # çº¦æŸæ¡ä»¶: ç¡®ä¿ n_heads èƒ½è¢« d_model æ•´é™¤
  while true; do
    nh=${n_heads_options[$((RANDOM % ${#n_heads_options[@]}))]}
    if [ $((dm % nh)) -eq 0 ]; then
      break
    fi
  done

  # --- GPU åˆ†é… (è½®è¯¢æœºåˆ¶) ---
  gpu_idx=$((job_count % NUM_GPUS))
  gpu_id=${GPUS[$gpu_idx]}

  # --- æ„å»ºå”¯ä¸€çš„æ¨¡å‹ID, ç”¨äºæ—¥å¿—å’Œæ¨¡å‹ä¿å­˜ ---
  model_id="${model_name}_${dataset_name}_sl${seq_len}_plen${pred_len}_dm${dm}_nh${nh}_nl${nl}_pl${pl}_st${st}_dff${dff}_dp${dp}_lr${lr}_bs${bs}_run${i}"

  echo "-----------------------------------------------------------------------"
  echo "ğŸ“ˆ å¯åŠ¨ä»»åŠ¡ [${i}/${TOTAL_RUNS}] -> åˆ†é…è‡³ GPU ${gpu_id}"
  echo "   Config: d_model=${dm}, n_heads=${nh}, n_layers=${nl}, d_ff=${dff}, patch_len=${pl}, stride=${st}"
  echo "   Training: bs=${bs}, lr=${lr}, dropout=${dp}"
  echo "   Model ID: ${model_id}"
  echo "-----------------------------------------------------------------------"

  # --- åœ¨åå°å¯åŠ¨è®­ç»ƒä»»åŠ¡ ---
  CUDA_VISIBLE_DEVICES=${gpu_id} python main.py \
    --is_training 1 \
    --model_id "$model_id" \
    --model_name "$model_name" \
    --d_model "$dm" \
    --n_heads "$nh" \
    --n_layers "$nl" \
    --d_ff "$dff" \
    --dropout "$dp" \
    --patch_len "$pl" \
    --patch_stride "$st" \
    --dataset_root_path "$dataset_root_path" \
    --dataset_name "$dataset_name" \
    --features M \
    --seq_len "$seq_len" \
    --pred_len "$pred_len" \
    --enc_in "$enc_in" \
    --dec_in "$dec_in" \
    --c_out "$c_out" \
    --loss "MSE" \
    --train_epochs 300 \
    --patience 10 \
    --val_interval 1 \
    --itr 5 \
    --batch_size "$bs" \
    --learning_rate "$lr" \
    --use_multi_gpu 0 &

  job_count=$((job_count + 1))
  sleep 2

  if [[ $(jobs -r -p | wc -l) -ge $NUM_GPUS ]]; then
      wait -n
  fi

done

echo "âœ… æ‰€æœ‰ ${TOTAL_RUNS} ç»„ä»»åŠ¡å·²å¯åŠ¨ï¼Œç­‰å¾…æœ€åæ‰¹æ¬¡çš„ä»»åŠ¡æ‰§è¡Œå®Œæ¯•..."
wait
echo "ğŸ‰ğŸ‰ğŸ‰ HYPE-Former åœ¨ P12 ä¸Šçš„éšæœºæœç´¢ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆï¼ğŸ‰ğŸ‰ğŸ‰"